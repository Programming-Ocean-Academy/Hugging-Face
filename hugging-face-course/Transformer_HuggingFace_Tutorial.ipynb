{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUCckbCaepTU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "MhnAdsYNfQDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "M4YilMJjiKMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"unsure\"\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
        "input_ids"
      ],
      "metadata": {
        "id": "P1qCoalFfZdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids[0])"
      ],
      "metadata": {
        "id": "cJd5VGE9f-mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"unbelivable\"\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "input_ids"
      ],
      "metadata": {
        "id": "O5QiebZ3gGMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in input_ids[0]:\n",
        "    print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "id": "2Ua9iQUxhC6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"homoscedasticity\"\n",
        "my_ids = tokenizer(word, return_tensors=\"pt\").input_ids\n",
        "my_ids"
      ],
      "metadata": {
        "id": "b5hBghdshfKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(my_ids.squeeze())"
      ],
      "metadata": {
        "id": "HpE73rR9h8bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"pneumonoultramicroscopicsilicovolcanoconiosis\"\n",
        "my_ids = tokenizer(word, return_tensors=\"pt\").input_ids\n",
        "# len(my_ids[0])\n",
        "my_ids"
      ],
      "metadata": {
        "id": "YlKTq2QsiIcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in my_ids.squeeze():\n",
        "    print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "id": "fS5hPRovi61X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"antidisestablishmentarianism\"\n",
        "token_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "token_ids, len(token_ids[0])"
      ],
      "metadata": {
        "id": "OgbPaHgMjNV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"floccinaucinihilipilification\"\n",
        "\n",
        "my_ids = tokenizer(word, return_tensors=\"pt\").input_ids\n",
        "my_ids, len(my_ids[0])"
      ],
      "metadata": {
        "id": "ZYvzloagjhyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in my_ids[0]:\n",
        "    print(tokenizer.decode(token_id))"
      ],
      "metadata": {
        "id": "5OWIfn-qjwhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "UAanQqJ0jy4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "HC8HEwYCkBzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2"
      ],
      "metadata": {
        "id": "-vBv9O_fkGOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I like machine learning to be able to predict the future.\"\n",
        "# Data Processing technique\n",
        "token_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = gpt2(token_ids).logits[0, -1]\n",
        "tokenizer.decode(outputs.argmax())"
      ],
      "metadata": {
        "id": "F3v1E2tNkZRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I learn machine learning to enhance our understanding of the world around us.\"\n",
        "\n",
        "token_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = gpt2(token_ids).logits[0, -1]\n",
        "tokenizer.decode(outputs.argmax())"
      ],
      "metadata": {
        "id": "1mnWrABfeopy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I learn machine learning to enhance\"\n",
        "token_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "outputs = gpt2(token_ids).logits[0, -1]\n",
        "final_logits = torch.topk(outputs, 20) # Feel free to play around with the K\n",
        "\n",
        "for index in final_logits.indices:\n",
        "    print(tokenizer.decode(index))"
      ],
      "metadata": {
        "id": "FBVDXok1fZ_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(final_logits.values, dim=0).sum()"
      ],
      "metadata": {
        "id": "0Mt1lI5Yhuc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(final_logits.values, dim=0).sum()"
      ],
      "metadata": {
        "id": "1Np7RsXTk37N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(logits):\n",
        "    \"\"\"Return token index with maximum probability.\"\"\"\n",
        "    return torch.argmax(logits, dim=-1)\n",
        "\n",
        "# TOP K SAMPLING\n",
        "\n",
        "def top_k_sampling(logits, k=50):\n",
        "    \"\"\"\n",
        "    keeps only top-k logits, normalize them into probability.\n",
        "    them sample one token from the filtered distribution.\n",
        "    \"\"\"\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    probs = F.softmax(values, dim=-1)\n",
        "    sampled = torch.multinomial(probs, 1)\n",
        "    return indices[sampled]\n",
        "\n",
        "# Top-p (Nuecles) Sampling\n",
        "\n",
        "def top_p_sampling(logits, p=0.9):\n",
        "    \"\"\"\n",
        "    Sort tokens by probability, keep smallest number whose culumative\n",
        "    probability exceeds threshold p, then sample one token.\n",
        "    \"\"\"\n",
        "\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "    cumulative_probs = sorted_probs.cumsum(dim=-1)\n",
        "\n",
        "    # Mask token outside nuclues\n",
        "    mask = cumulative_probs > p\n",
        "    sorted_logits[mask] = float(\"-inf\")\n",
        "\n",
        "    # Sample from filtered logits\n",
        "    filtered_probs = F.softmax(sorted_logits, dim=-1)\n",
        "    sampled = torch.multinomial(filtered_probs, 1)\n",
        "\n",
        "    # Return token index in originial vocabulary\n",
        "    return sorted_indices[sampled]\n",
        "\n",
        "## Temperature Sampling ##\n",
        "\n",
        "def temperature_sampling(logits, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Scale logits by temperature before sampling.\n",
        "    Lower temperature => sharper distribution\n",
        "    \"\"\"\n",
        "\n",
        "    scaled = logits / temperature\n",
        "    probs = F.softmax(scaled, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "\n",
        "## Random Sampling ##\n",
        "\n",
        "def random_sampling(logits):\n",
        "    \"\"\"\n",
        "    Sample dirctly from softmax distribution without filtring\n",
        "    \"\"\"\n",
        "\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, 1)\n",
        "\n",
        "# sentence = \"Today I decided to go to the local library and find out what was in my wallet.\"\n",
        "sentence = \"I am really happy becuase I have gone back in time.\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "output = gpt2(**inputs)\n",
        "logits = output.logits[0, -1]\n",
        "\n",
        "print(f\"Greedy Decode: \", tokenizer.decode([greedy_decode(logits)]))\n",
        "print(f\"Top-K Sampling: \", tokenizer.decode(top_k_sampling(logits, k=10)))\n",
        "print(f\"Top-P-Sampling: \", tokenizer.decode(top_p_sampling(logits, p=0.9)))\n",
        "print(f\"Temp: \", tokenizer.decode(temperature_sampling(logits, temperature=1)))\n",
        "print(f\"Radnom: \", tokenizer.decode(random_sampling(logits)))"
      ],
      "metadata": {
        "id": "8k-CPEC7i88V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(top_k_sampling(outputs))"
      ],
      "metadata": {
        "id": "ZQDUFPoOlVse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "qSRwZCfRo0sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(top_p_sampling(outputs, p=0.9))"
      ],
      "metadata": {
        "id": "8uHux5VcozNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(temperature_sampling(outputs, temperature=1.5))"
      ],
      "metadata": {
        "id": "kZX2BHB1qPvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(random_sampling(outputs))"
      ],
      "metadata": {
        "id": "F6YX6YCvrZUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxjHLrUxE6SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I learn machine learning to enhance our understanding of the brain in\"\n",
        "token_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "outputs = gpt2(token_ids).logits # Raw Unnormlized Score - Values\n",
        "outputs = torch.softmax(outputs[0, -1], dim=-1)\n",
        "\n",
        "top10 = torch.topk(outputs, k=10)\n",
        "\n",
        "for index, value in zip(top10.indices, top10.values):\n",
        "    print(f\"{tokenizer.decode(index)} -- {value:.1%}\")"
      ],
      "metadata": {
        "id": "2CMn6YnfE77I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "V_UXeLtkE9EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "w-68sZ9AJz21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is machine learning?\"\n",
        "output = pipe(prompt)"
      ],
      "metadata": {
        "id": "UhYA7VTCME1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "WyorpKYIMYBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "YSJIyKJhM_3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "V8ibkb17NBIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "ucNTDpCElRiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "20waNI9PlOfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(ds)"
      ],
      "metadata": {
        "id": "vFy3a7DflOX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "dIM_fiL9qCqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"]"
      ],
      "metadata": {
        "id": "2bg0uKHdqM2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "q1bWzy7nqSHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "2Rbf-bfIqUDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_df = ds[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "pwGOLNC3qcpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_df[\"text\"]"
      ],
      "metadata": {
        "id": "q2L0FSjdqgzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(my_dataset_df[\"text\"])"
      ],
      "metadata": {
        "id": "2mAGcUtVqkNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "4KfoK3G_qm4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "cwsjBSD6qqAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This day is great!\")"
      ],
      "metadata": {
        "id": "kFnRryxaq4FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This day is terrible and i am so sad\")[0][\"label\"]"
      ],
      "metadata": {
        "id": "XmQlw6-KrCLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(review_text):\n",
        "    return classifier(review_text[:500])[0][\"label\"]"
      ],
      "metadata": {
        "id": "E5Mt-tbsrK0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_df[\"model_prediction\"] = my_dataset_df[\"text\"].apply(score)"
      ],
      "metadata": {
        "id": "L5-Nw-2XrZLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_df[[\"label\", \"model_prediction\"]][:20]"
      ],
      "metadata": {
        "id": "g9Hem0L5wmD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_df.iloc[0]"
      ],
      "metadata": {
        "id": "qaQLnv6wxiAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = my_dataset_df.iloc[0][\"text\"]\n",
        "classifier(review)[0][\"label\"]"
      ],
      "metadata": {
        "id": "CaR1baoZxN9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "9iYFBJb4xnn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")"
      ],
      "metadata": {
        "id": "kYQN2VjIx9AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The company reported a strong increase in quarterly revnue, exceeding market expectations.\"\n",
        "finbert(sentence)"
      ],
      "metadata": {
        "id": "PLke-rPuyCw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Shares fell after the firm reported lower-than-expected earnings\"\n",
        "finbert(sentence)"
      ],
      "metadata": {
        "id": "sVI66pe2yk1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Strong consumer demand drove record sales across all regions\",\n",
        "             \"Supply chain disruptions severly affected production output\"]"
      ],
      "metadata": {
        "id": "Bw04Tjiiy0cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert(sentences)"
      ],
      "metadata": {
        "id": "qNK1vzyKy_X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition"
      ],
      "metadata": {
        "id": "c4N0w3G0zGPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Apple announced record earnings in the United States on Monday.\""
      ],
      "metadata": {
        "id": "EZetl5JCzIUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\")"
      ],
      "metadata": {
        "id": "Pr-Bcr-8zVwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "id": "7XhA-V-1zx7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner(sentence)"
      ],
      "metadata": {
        "id": "YBhqvtVczfVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I live in UK worked at Facebook after graduating from Harvard\""
      ],
      "metadata": {
        "id": "PrW7vrSBz5KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner(sentence)"
      ],
      "metadata": {
        "id": "8q_jOgjrz9HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "TMBPfjll0BUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bot = pipeline(\"question-answering\")"
      ],
      "metadata": {
        "id": "4bPagYxv0HlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Financial sentiment analysis is a challenging task due to the specialized\n",
        "language and lack of labeled data in that domain. General-purpose models are\n",
        "not effective enough because of the specialized language used in a financial\n",
        "context. We hypothesize that pre-trained language models can help with this\n",
        "problem because they require fewer labeled examples and they can be further\n",
        "trained on domain-specific corpora. We introduce FinBERT, a language model\n",
        "based on BERT, to tackle NLP tasks in the financial domain. Our results show\n",
        "improvement in every measured metric on current state-of-the-art results for\n",
        "two financial sentiment analysis datasets. We find that even with a smaller\n",
        "training set and fine-tuning only a part of the model, FinBERT outperforms\n",
        "state-of-the-art machine learning methods.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nSc7NGg20Zvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is financial sentiment analysis?\""
      ],
      "metadata": {
        "id": "leqSG8yn0oXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bot(question=question, context=context)"
      ],
      "metadata": {
        "id": "sin7AxzX0jhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is FinBERT?\""
      ],
      "metadata": {
        "id": "kONNc66c0uUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_bot(question=question, context=context)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "id": "hQ_Cg8Iz053Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation"
      ],
      "metadata": {
        "id": "v2qRMO461MAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translater = pipeline(\"translation_en_to_fr\")"
      ],
      "metadata": {
        "id": "U5nMYQJg1Ync"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translater(\"Hello\")"
      ],
      "metadata": {
        "id": "tWCZKKKY1rlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translater(\"Thanks\")"
      ],
      "metadata": {
        "id": "ecJn6oq61xom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What is your name?\"\n",
        "translater(sentence)[0][\"translation_text\"]"
      ],
      "metadata": {
        "id": "tkDrP9BT10Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\")"
      ],
      "metadata": {
        "id": "a4-hqdcd17ft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}